{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68cbedc9",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Hopper Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d761ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodule_Reinforce_ActorCritic\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrac\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\letig\\Desktop\\MachineLearning\\progetto\\ReinforcementLearningProject\\module_Reinforce_ActorCritic.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#Problem specific\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import module_Reinforce_ActorCritic as rac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f1d936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\letig\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\letig\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\letig\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\letig\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\letig\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af4e70",
   "metadata": {},
   "source": [
    "## Actor-Critic & Reinforce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e57e3",
   "metadata": {},
   "source": [
    "### Train method\n",
    "\n",
    "`train` method parameters:\n",
    "- `type_alg`: Algorithm type.\n",
    "    - 0: Reinforce;\n",
    "    - 1: Actor-Critic with different classes for policy and value and different optimizers;\n",
    "    - 2: Actor-Critic with single class for policy and value and single optimizer of losses sum.\n",
    "\n",
    "- `hopper`: Environment in which the policy is trained.\n",
    "    - 'S': Source Custom Hopper (default);\n",
    "    - 'T': Target Custom Hopper. \n",
    "- `n_episodes`: Number of episodes for training. Default: 5e4.\n",
    "- `trained_model`: Model from which the training starts. If `None` (default) the training is from scratch.\n",
    "- `baseline`: Value of the baseline for the Reinforce algorithm. Considered only if `type_alg`=0. Default:0.\n",
    "- `starting_threshold`: None <span style=\"color:red\"> TODO </span>\n",
    "- `save_every`: 75 <span style=\"color:red\"> TODO </span>\n",
    "- `print_every`: Every `print_every` episodes the number and the return of the current episode is printed. It is suggested to be set to around 1/5 of `n_episodes`. Default: 1e4.\n",
    "- `print_name`: Boolean parameter. Print the name of the model when the training ends. Default: True.\n",
    "- `plot`: Boolean parameter. Print the graph of episodes' returns when the training ends. Default: True.\n",
    "- `random_state`: Random seed. Default: 42.\n",
    "- `device`: Device to use for training. Default: 'cpu'\n",
    "\n",
    "`train` method returns:  <span style=\"color:red\"> TODO </span>\n",
    "- `numbers`:\n",
    "\n",
    "- `returns_array`:\n",
    "- `times_array`:\n",
    "- `model_name`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91bdba82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrac\u001b[49m\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m2\u001b[39m,hopper\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m, n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e4\u001b[39m, trained_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, baseline\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, starting_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m700\u001b[39m, save_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m75\u001b[39m, print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e4\u001b[39m, print_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rac' is not defined"
     ]
    }
   ],
   "source": [
    "rac.train(2,hopper='S', n_episodes=5e4, trained_model=None, baseline=0, starting_threshold=700, save_every=75, print_every=1e4, print_name=True, plot=True, random_state=42, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c37b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
